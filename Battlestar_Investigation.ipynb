{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Battlestar Investigation ðŸš€"
      ],
      "metadata": {
        "id": "Hw_d6b-45vNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "tTOImo3V6Hlk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h5gOhoP42wu"
      },
      "outputs": [],
      "source": [
        "# Install Neel Nanda's Easy Transformer\n",
        "import os\n",
        "os.system(\"pip install git+https://github.com/neelnanda-io/Easy-Transformer.git\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "# from google.colab import drive\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from functools import *\n",
        "import pandas as pd\n",
        "import gc\n",
        "import collections\n",
        "import copy\n",
        "\n",
        "# import comet_ml\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets"
      ],
      "metadata": {
        "id": "3PuKNDre5TGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from easy_transformer.utils import (\n",
        "    gelu_new,\n",
        "    to_numpy,\n",
        "    get_corner,\n",
        "    lm_cross_entropy_loss,\n",
        ")  # Helper functions\n",
        "from easy_transformer.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from easy_transformer import EasyTransformer, EasyTransformerConfig\n",
        "import easy_transformer\n",
        "from easy_transformer.experiments import (\n",
        "    ExperimentMetric,\n",
        "    AblationConfig,\n",
        "    EasyAblation,\n",
        "    EasyPatching,\n",
        "    PatchingConfig,\n",
        ")"
      ],
      "metadata": {
        "id": "6BIRoiaP5izM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "7bxjAMZN6JqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "NcFUnaXb5m1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'stanford-crfm/battlestar-gpt2-small-x49'\n",
        "model = EasyTransformer.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "id": "UzPY3a6o5o2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(False)"
      ],
      "metadata": {
        "id": "tEwTxE8S6VDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting functions\n",
        "# This is mostly a bunch of over-engineered mess to hack Plotly into producing \n",
        "# the pretty pictures I want, I recommend not reading too closely unless you \n",
        "# want Plotly hacking practice\n",
        "def to_numpy(tensor, flat=False):\n",
        "    if type(tensor)!=torch.Tensor:\n",
        "        return tensor\n",
        "    if flat:\n",
        "        return tensor.flatten().detach().cpu().numpy()\n",
        "    else:\n",
        "        return tensor.detach().cpu().numpy()\n",
        "def imshow(tensor, xaxis=None, yaxis=None, animation_name='Snapshot', **kwargs):\n",
        "    tensor = torch.squeeze(tensor)\n",
        "    px.imshow(to_numpy(tensor, flat=False), \n",
        "              labels={'x':xaxis, 'y':yaxis, 'animation_name':animation_name}, \n",
        "              **kwargs).show()\n",
        "# Set default colour scheme\n",
        "# Creates good defaults for showing divergent colour scales (ie with both \n",
        "# positive and negative values, where 0 is white)\n",
        "imshow = partial(imshow, color_continuous_scale='RdBu', color_continuous_midpoint=0.0)\n",
        "\n",
        "def line(x, y=None, hover=None, xaxis='', yaxis='', **kwargs):\n",
        "    if type(y)==torch.Tensor:\n",
        "        y = to_numpy(y, flat=True)\n",
        "    if type(x)==torch.Tensor:\n",
        "        x=to_numpy(x, flat=True)\n",
        "    fig = px.line(x, y=y, hover_name=hover, **kwargs)\n",
        "    fig.update_layout(xaxis_title=xaxis, yaxis_title=yaxis)\n",
        "    fig.show()\n",
        "def scatter(x, y, **kwargs):\n",
        "    px.scatter(x=to_numpy(x, flat=True), y=to_numpy(y, flat=True), **kwargs).show()\n",
        "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
        "    # Helper function to plot multiple lines\n",
        "    if type(lines_list)==torch.Tensor:\n",
        "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
        "    if x is None:\n",
        "        x=np.arange(len(lines_list[0]))\n",
        "    fig = go.Figure(layout={'title':title})\n",
        "    fig.update_xaxes(title=xaxis)\n",
        "    fig.update_yaxes(title=yaxis)\n",
        "    for c, line in enumerate(lines_list):\n",
        "        if type(line)==torch.Tensor:\n",
        "            line = to_numpy(line)\n",
        "        if labels is not None:\n",
        "            label = labels[c]\n",
        "        else:\n",
        "            label = c\n",
        "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
        "    if log_y:\n",
        "        fig.update_layout(yaxis_type=\"log\")\n",
        "    fig.show()"
      ],
      "metadata": {
        "id": "7gNQlgrt9cUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0: Verifying that the model can do IOI"
      ],
      "metadata": {
        "id": "wTPLP_yr710H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_name)\n",
        "example_prompt = \"While John and Mary were walking their dog, Mary handed the leash to\" # @param\n",
        "example_answer = \" John\" #@param\n",
        "# Hacky function to map a text string to separate tokens as text\n",
        "example_prompt_str_tokens = model.to_str_tokens(example_prompt, prepend_bos=True)\n",
        "example_answer_str_tokens = model.to_str_tokens(example_answer)\n",
        "print(\"Tokenized prompt:\", example_prompt_str_tokens)\n",
        "print(\"Tokenized answer:\", example_answer_str_tokens)\n",
        "prompt_length = len(example_prompt_str_tokens)\n",
        "answer_length = len(example_answer_str_tokens)\n",
        "example_logits = model(example_prompt+example_answer)\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    print(\"Logits for token:\", example_answer_str_tokens[index - prompt_length])\n",
        "    token_logits = example_logits[0, index-1]\n",
        "    probs = torch.nn.functional.softmax(token_logits, dim=-1)\n",
        "    values, indices = token_logits.sort(descending=True)\n",
        "    for i in range(10):\n",
        "        print(f\"Top {i}th logit. Logit: {values[i].item():.6} Prob: {probs[indices[i]].item():.2%} Token: |{model.tokenizer.decode(indices[i])}|\")"
      ],
      "metadata": {
        "id": "_6_m0Hdr7ZmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"While John and Mary were walking their dog, Mary handed the leash to\"\n",
        "#example_text_reverse = \"While John and Mary were walking their dog, John handed the leash to\"\n",
        "john_index = model.tokenizer.encode(\" John\")[0]\n",
        "mary_index = model.tokenizer.encode(\" Mary\")[0]\n",
        "print(model_name)\n",
        "def get_logit_diff(logits):\n",
        "    # Takes in a batch x position x vocab tensor of logits, and returns the difference between the John and Mary logit\n",
        "    return logits[0, -1, john_index] - logits[0, -1, mary_index]\n",
        "example_logits = model(example_text) # Shape batch x position x vocab\n",
        "example_logit_diff = get_logit_diff(example_logits)\n",
        "#example_logits_reverse = model(example_text_reverse) # Shape batch x position x vocab\n",
        "#example_logit_diff_reverse = get_logit_diff(example_logits_reverse)\n",
        "print(f\"Input text: {example_text}, John logit - Mary logit: {example_logit_diff.item()}\")\n",
        "#print(f\"Input text: {example_text_reverse}, John logit - Mary logit: {example_logit_diff_reverse.item()}\")"
      ],
      "metadata": {
        "id": "mMO0TBEx6ybe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Direct Logit Attribution"
      ],
      "metadata": {
        "id": "8pAr0VA47_ks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the model on the example text and cache all activations in example_cache, for future use"
      ],
      "metadata": {
        "id": "-r1WIrbhyv13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.cfg.use_attn_result = True\n",
        "example_cache = {}\n",
        "model.cache_all(example_cache, remove_batch_dim=True)\n",
        "_ = model(example_text)\n",
        "model.reset_hooks()\n",
        "model.cfg.use_attn_result = False\n",
        "for act_name in example_cache:\n",
        "    print(act_name, example_cache[act_name].shape)"
      ],
      "metadata": {
        "id": "z2m_s2kg9Mml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set use attn result to True - this gives us a hook for the result of each head, \n",
        "# ie the d_model length vectors whose sum makes up attn_out and is added to the residual stream\n",
        "print(model_name)\n",
        "model.cfg.use_attn_result = True\n",
        "\n",
        "logit_diff_direction = model.unembed.W_U[:, john_index] - model.unembed.W_U[:, mary_index]\n",
        "# Take the scaling factor of the layernorm pre-unembed on the final token, so our logit attrs are on the same scale\n",
        "final_layer_norm_scale = (example_cache['ln_final.hook_scale'][0, -1])\n",
        "print(\"Final layer norm scaling factor:\", final_layer_norm_scale.item())\n",
        "\n",
        "direct_logit_attr = torch.zeros(model.cfg.n_layers, model.cfg.n_heads).to(device)\n",
        "def calc_direct_logit_attr(result, hook):\n",
        "    layer = int(hook.name.split('.')[1])\n",
        "    final_token_result = result[0, -1]\n",
        "    direct_logit_attr[layer] = (final_token_result @ (logit_diff_direction))/final_layer_norm_scale\n",
        "\n",
        "model.run_with_hooks(example_text, fwd_hooks = [(lambda name:name.endswith('hook_result'), calc_direct_logit_attr)])\n",
        "\n",
        "imshow(direct_logit_attr, xaxis='Head', yaxis='Layer', title='Direct Logit Attribution')\n",
        "\n",
        "# Switch use_attn_result back off, since it consumes a lot of memory (Exercise: Why is this more expensive than eg calculating the value?)\n",
        "model.cfg.use_attn_result = False"
      ],
      "metadata": {
        "id": "WEXIXTpl79nZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Using Attention Patterns"
      ],
      "metadata": {
        "id": "i6puOzHLMYWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_hook_attn(name):\n",
        "    split_name = name.split(\".\")\n",
        "    return split_name[-1] == \"hook_attn\""
      ],
      "metadata": {
        "id": "-zwIHR-KMdG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"A robot may not injure a human being or, through inaction, allow a human being to come to harm. \\\n",
        "A robot must obey the orders given it by human beings except where such orders would conflict with the First Law. \\\n",
        "A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.\"\n",
        "\n",
        "print(\"Long text:\", long_text)\n",
        "# We first cache attention patterns\n",
        "attn_cache = {}\n",
        "\n",
        "\n",
        "def cache_attn(attn, hook):\n",
        "    attn_cache[hook.name] = attn\n",
        "\n",
        "\n",
        "logits = model.run_with_hooks(long_text, fwd_hooks=[(filter_hook_attn, cache_attn)])\n",
        "\n",
        "# We then go through the cache and find the average attention paid to previous tokens\n",
        "prev_token_scores = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
        "for layer in range(model.cfg.n_layers):\n",
        "    for head in range(model.cfg.n_heads):\n",
        "        attn = attn_cache[f\"blocks.{layer}.attn.hook_attn\"][0, head]\n",
        "        prev_token_scores[layer, head] = attn.diag(-1).mean().item()\n",
        "\n",
        "px.imshow(\n",
        "    prev_token_scores,\n",
        "    x=[f\"Head {hi}\" for hi in range(model.cfg.n_heads)],\n",
        "    y=[f\"Layer {i}\" for i in range(model.cfg.n_layers)],\n",
        "    title=\"Prev Token Scores\",\n",
        "    color_continuous_scale=\"Blues\",\n",
        ")"
      ],
      "metadata": {
        "id": "0sBrKKggLQF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Using Ablation Techniques"
      ],
      "metadata": {
        "id": "7tNEXM5qcspO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a. Knocking out a single head"
      ],
      "metadata": {
        "id": "h30kLwStk1F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model.reset_hooks()\n",
        "def ablate_head_hook(value, hook, head_index):\n",
        "    # Shape of value: batch x position x head_index x d_head\n",
        "    value[:, :, head_index] = 0.\n",
        "    return value\n",
        "\n",
        "layer = 10\n",
        "head_index = 7\n",
        "\n",
        "example_prompt = \"While John and Mary were walking their dog, Mary handed the leash to\" # @param\n",
        "example_answer = \" John\" #@param\n",
        "# Hacky function to map a text string to separate tokens as text\n",
        "example_prompt_str_tokens = model.to_str_tokens(example_prompt, prepend_bos=True)\n",
        "example_answer_str_tokens = model.to_str_tokens(example_answer)\n",
        "prompt_length = len(example_prompt_str_tokens)\n",
        "answer_length = len(example_answer_str_tokens)\n",
        "\n",
        "example_logits = model.run_with_hooks(example_prompt, fwd_hooks=[(f\"blocks.{layer}.attn.hook_v\", partial(ablate_head_hook, head_index=head_index))])\n",
        "\n",
        "def get_logit_diff(logits):\n",
        "    # Takes in a batch x position x vocab tensor of logits, and returns the difference between the John and Mary logit\n",
        "    return logits[0, -1, john_index] - logits[0, -1, mary_index]\n",
        "\n",
        "john_index = model.tokenizer.encode(\" John\")[0]\n",
        "mary_index = model.tokenizer.encode(\" Mary\")[0]\n",
        "\n",
        "for index in range(prompt_length, prompt_length + answer_length):\n",
        "    token_logits = example_logits[0, index-1]\n",
        "    probs = torch.nn.functional.softmax(token_logits, dim=-1)\n",
        "    values, indices = token_logits.sort(descending=True)\n",
        "    for i in range(2):\n",
        "        print(f\"Top {i}th logit. Logit: {values[i].item():.6} Prob: {probs[indices[i]].item():.2%} Token: |{model.tokenizer.decode(indices[i])}|\")\n",
        "    example_logit_diff = get_logit_diff(example_logits)\n",
        "    print(f\"Input text: {example_prompt}, John logit - Mary logit: {example_logit_diff.item()}\")\n",
        "#model.reset_hooks()"
      ],
      "metadata": {
        "id": "hX3LFDuScsBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b. Ablating Each Head and Seeing Effect"
      ],
      "metadata": {
        "id": "Z3HZwJRCk60M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ablate_head_hook(value, hook, head_index):\n",
        "    # Shape of value: batch x position x head_index x d_head\n",
        "    value[:, :, head_index] = 0.\n",
        "    return value\n",
        "\n",
        "example_prompt = \"While John and Mary were walking their dog, Mary handed the leash to\" # @param\n",
        "example_answer = \" John\" #@param\n",
        "# Hacky function to map a text string to separate tokens as text\n",
        "example_prompt_str_tokens = model.to_str_tokens(example_prompt, prepend_bos=True)\n",
        "example_answer_str_tokens = model.to_str_tokens(example_answer)\n",
        "prompt_length = len(example_prompt_str_tokens)\n",
        "answer_length = len(example_answer_str_tokens)\n",
        "\n",
        "head_ablation = torch.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
        "logit_diff_history = []\n",
        "for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
        "    for head_index in range(model.cfg.n_heads):\n",
        "      example_logits = model.run_with_hooks(example_prompt, fwd_hooks=[(f\"blocks.{layer}.attn.hook_v\", partial(ablate_head_hook, head_index=head_index))])\n",
        "      for index in range(prompt_length, prompt_length + answer_length):\n",
        "        token_logits = example_logits[0, index-1]\n",
        "        probs = torch.nn.functional.softmax(token_logits, dim=-1)\n",
        "        values, indices = token_logits.sort(descending=True)\n",
        "        example_logit_diff = get_logit_diff(example_logits)\n",
        "        #print(f\"Layer: {layer}, Head: {head_index}, Input text: {example_prompt}, John logit - Mary logit: {example_logit_diff.item()}\")\n",
        "        logit_diff_history.append(example_logit_diff.item())\n"
      ],
      "metadata": {
        "id": "lTFAMkvLqbsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(logit_diff_history)\n",
        "plt.ylabel('John - Mary Logit Difference')\n",
        "plt.xlabel('Layer + Head Index (Layer First)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4VDBK0yvuSUr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}